{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b844fc53-7617-4e66-9711-61b9fb7e6cf9",
   "metadata": {},
   "source": [
    "# ML Project 3- Suraj Jayakumar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d3bd6-00b8-434a-b583-739d5438d490",
   "metadata": {},
   "source": [
    "# 1) Collaborative Filtering on netflix dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe4839-e403-4b6b-bea1-daa7f2be474f",
   "metadata": {},
   "source": [
    "## Step1: Import training ratings and create the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed816f-5385-4766-b0b3-39bc18c0a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"TrainingRatings.txt\", names=['movieID', 'userID', 'rating'])\n",
    "df_filtered = df[df['rating'] != 0]\n",
    "user_counts = df_filtered.groupby('userID').size().values\n",
    "user_rating_sums = df.groupby('userID')['rating'].sum().values\n",
    "average_ratings = np.divide(user_rating_sums, user_counts, out=np.zeros_like(user_rating_sums), where=user_counts!=0)\n",
    "\n",
    "# Create a user-movie rating matrix\n",
    "user_movie_matrix = df.pivot(index='userID', columns='movieID', values='rating').fillna(0).values\n",
    "\n",
    "# Center the ratings by subtracting the average rating for each user\n",
    "centered_ratings = user_movie_matrix - average_ratings[:, np.newaxis]\n",
    "centered_ratings[user_movie_matrix==0]=0\n",
    "\n",
    "#computing the denominator\n",
    "squared_sum = np.einsum('ij,ij->i', centered_ratings, centered_ratings)\n",
    "rooted_squared_sum = np.sqrt(squared_sum)\n",
    "denominator=rooted_squared_sum*rooted_squared_sum.T\n",
    "#computing the numerator\n",
    "numerator = np.einsum('ij,kj->ik', centered_ratings, centered_ratings)\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        similarity_matrix=numerator/denominator\n",
    "        similarity_matrix[denominator == 0] = 0  # Set undefined correlations to 0\n",
    "\n",
    "# Set diagonal to 0\n",
    "np.fill_diagonal(similarity_matrix, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64899c2-d2c5-4c78-897e-c2afc7e54602",
   "metadata": {},
   "source": [
    "## Step 2: create a prediction matrix using the user's average rating and summing it with the weighted ratings of other users for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a214f0b5-3bd8-4dc6-9c4e-a7a590d76c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing undefined values from the correlation matrix\n",
    "similarity_matrix=np.nan_to_num(similarity_matrix)\n",
    "centered_ratings=np.nan_to_num(centered_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b96d2c77-fbae-46ed-8ba3-0ddf4ef9ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix = average_ratings[:, np.newaxis] + 1e-5*np.dot(similarity_matrix, centered_ratings) \n",
    "# 10^-5 multiplied for normalization: obtained empirically in this case via hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079af408-d990-48fa-9a83-48c1489b205d",
   "metadata": {},
   "source": [
    "## Step 3: Find mean absolute error and root mean squared error by testing the predictions on user and movie combinations in the testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51ee44ec-3873-4717-918a-08f8347dcfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.7883677257380779\n",
      "Root Mean Squared Error (RMSE): 0.9856831548353928\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "df_test = pd.read_csv(\"TestingRatings.txt\", names=['movieID', 'userID', 'rating'])\n",
    "user_id_to_index = {user_id: index for index, user_id in enumerate(df.groupby('userID').size().index)}\n",
    "# Get predictions for test ratings\n",
    "predictions = []\n",
    "actual_ratings = []\n",
    "counter=0;\n",
    "\n",
    "for _, row in df_test.iterrows():\n",
    "    movie_id = row['movieID']\n",
    "    user_id = row['userID']\n",
    "    if user_id in user_id_to_index:\n",
    "      user_index = user_id_to_index[user_id]\n",
    "      movie_index = np.where(df['movieID'].unique() == movie_id)[0]\n",
    "      if movie_index.size > 0:\n",
    "          prediction = prediction_matrix[user_index, movie_index[0]]\n",
    "          predictions.append(prediction)\n",
    "          actual_ratings.append(row['rating'])\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the predictions\n",
    "mae=mean_absolute_error(actual_ratings, predictions)\n",
    "mse = mean_squared_error(actual_ratings, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e194c6d-4809-4903-8bc4-a0b9ecb1c7a8",
   "metadata": {},
   "source": [
    "# 2) Scikit-learn's SVM classifier on the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6b8a69-e1c4-4679-bd0b-10039d322115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X / 255.\n",
    "# rescale the data, use the traditional train/test split\n",
    "# (60K: Train) and (10K: Test)\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "#tuning hyperparameters which are kernel function and penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68619260-f7de-4537-8624-909190d4e77c",
   "metadata": {},
   "source": [
    "## 2.1) The regularization parameter C and the kernel type whether linear, gaussian or polynomial is varied. \n",
    "## 2.2) In the case of gaussian kernel we vary the gamma parameter which affects the curvature of the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e7fd5-d344-42cf-bc4c-8290bbc2ae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for kernel linear and C 0.1: 0.0528\n",
      "Error rate for kernel linear and C 1: 0.0596\n",
      "Error rate for kernel linear and C 10: 0.069\n",
      "Error rate for kernel linear and C 100: 0.0742\n",
      "Error rate for kernel rbf and C 0.1: 0.0405\n",
      "Error rate for kernel rbf, C 1 and gamma 0.01: 0.0231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "for kernel in ['linear', 'rbf','poly']:\n",
    "  for c in [0.1, 1, 10, 100]:\n",
    "    #get error rate for every combination of parameters\n",
    "    if kernel=='rbf' and c==1:\n",
    "        for gamma in [0.01,1,100]:\n",
    "            svc = SVC(kernel=kernel, C=c, gamma=gamma)\n",
    "            svc.fit(X_train, y_train)\n",
    "            y_pred = svc.predict(X_test)\n",
    "            error_rate = np.mean(y_pred != y_test)\n",
    "            print(\"Error rate for kernel {}, C {} and gamma {}: {}\".format(kernel, c, gamma,error_rate))\n",
    "        continue\n",
    "    svc = SVC(kernel=kernel, C=c)\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    error_rate = np.mean(y_pred != y_test)\n",
    "    print(\"Error rate for kernel {} and C {}: {}\".format(kernel, c, error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d620d-e66d-4548-ac37-9d8c85694c56",
   "metadata": {},
   "source": [
    "# 3) Scikit-learn's KNN classifier on the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a1417-074c-42a9-a725-7d03ade6880f",
   "metadata": {},
   "source": [
    "## 3.1) The number of neighbours considered is varied- 3, 10, 20.  \n",
    "## 3.2) different distance metrics are used for calculating weights.\n",
    "## 3.3) Algorithm used is varied whether it be brute force, Ball Tree, or KD- Tree\n",
    "## 3.4) Extreme cases of n=1 and n=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e3d186c-87b3-484b-9924-f90aedeb6d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for parameters {'n_neighbors': 1, 'weights': 'uniform'}: 0.0309\n",
      "Error rate for parameters {'n_neighbors': 50, 'weights': 'uniform'}: 0.0466\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "parameters = [\n",
    "    #varying number of nearest neighbours taken into account with uniform weights\n",
    "    {'n_neighbors': 3, 'weights': 'uniform'},\n",
    "    {'n_neighbors': 10, 'weights': 'uniform'},\n",
    "    {'n_neighbors': 20, 'weights': 'uniform'},\n",
    "\n",
    "    #varying distance metric\n",
    "    {'n_neighbors': 3, 'weights': 'distance', 'metric': 'cityblock'},\n",
    "    {'n_neighbors': 3, 'weights': 'distance', 'metric': 'euclidean'},\n",
    "    {'n_neighbors': 3, 'weights': 'distance', 'metric': 'cosine'},\n",
    "    \n",
    "    #varying algorithm used\n",
    "    {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'brute' },\n",
    "    {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'ball_tree'},\n",
    "\n",
    "    #trying extremely high and low nearest neighbour values\n",
    "    {'n_neighbors': 1, 'weights': 'uniform'},\n",
    "    {'n_neighbors': 50, 'weights': 'uniform'},\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "for param in parameters:\n",
    "  knn = KNeighborsClassifier(**param)\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  error_rate = np.mean(y_pred != y_test)\n",
    "  print(f\"Error rate for parameters {param}: {error_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47704ae0-9ae4-4166-b04f-d7df18b5f7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
